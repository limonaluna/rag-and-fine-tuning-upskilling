{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set Up Environment Variables\n",
    "To store credentials securely, rename the `.env.sample` file folder to `.env` in the same directory as the notebook and update the variables with the required connection information.\n",
    "\n",
    "### 2. Install Dependenices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from openai import AzureOpenAI\n",
    "from azure.search.documents import SearchClient\n",
    "import dotenv\n",
    "from azure.search.documents.models import VectorizedQuery, VectorizableTextQuery\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load environment variables and instantiate your OpenAI and AI Search clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Azure OpenAI environment variables\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_CHAT_COMPLETION_DEPLOYED_MODEL_NAME = os.getenv(\"AZURE_OPENAI_CHAT_COMPLETION_DEPLOYED_MODEL_NAME\")\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYED_MODEL_NAME = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYED_MODEL_NAME\")\n",
    "\n",
    "# Load Azure Search environment variables\n",
    "AZURE_SEARCH_ENDPOINT = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "AZURE_SEARCH_INDEX_NAME = os.getenv(\"AZURE_SEARCH_INDEX_NAME\")\n",
    "AZURE_SEARCH_ADMIN_KEY = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "\n",
    "# ðŸ”¹ Initialize Azure OpenAI Client (API Key or Managed Identity)\n",
    "if AZURE_OPENAI_API_KEY:\n",
    "    openai_client = AzureOpenAI(\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        api_version=\"2024-10-21\"\n",
    "    )\n",
    "else:\n",
    "    azure_credential = DefaultAzureCredential()\n",
    "    token_provider = get_bearer_token_provider(azure_credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "    openai_client = AzureOpenAI(\n",
    "        azure_ad_token_provider=token_provider,\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        api_version=\"2024-10-21\"\n",
    "    )\n",
    "\n",
    "# ðŸ”¹ Initialize Azure AI Search Client (API Key or Managed Identity)\n",
    "if AZURE_SEARCH_ADMIN_KEY:\n",
    "    search_client = SearchClient(\n",
    "        endpoint=AZURE_SEARCH_ENDPOINT,\n",
    "        index_name=AZURE_SEARCH_INDEX_NAME,\n",
    "        credential=AzureKeyCredential(AZURE_SEARCH_ADMIN_KEY)\n",
    "    )\n",
    "else:\n",
    "    azure_credential = DefaultAzureCredential()\n",
    "    search_client = SearchClient(\n",
    "        endpoint=AZURE_SEARCH_ENDPOINT,\n",
    "        index_name=AZURE_SEARCH_INDEX_NAME,\n",
    "        credential=azure_credential\n",
    "    )\n",
    "\n",
    "def get_embedding(text):\n",
    "    return openai_client.embeddings.create(\n",
    "        model=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYED_MODEL_NAME\"),\n",
    "        input=text\n",
    "    ).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify that you use the right endpoints\n",
    "print(f\"Azure OpenAI Endpoint: {AZURE_OPENAI_ENDPOINT}\")\n",
    "print(f\"AZURE_OPENAI_EMBEDDING_DEPLOYED_MODEL_NAME: {AZURE_OPENAI_EMBEDDING_DEPLOYED_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Prepare a question\n",
    "\n",
    "Define a sample question and convert it into an embedding vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"What is included in my Northwind Health Plus plan that is not in standard?\"\n",
    "user_question_vector = get_embedding(user_question)\n",
    "print(user_question_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Retrieve matching documents\n",
    "\n",
    "Perform a vector search in Azure AI Search to retrieve relevant document chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = search_client.search(\n",
    "    None,\n",
    "    top=3,\n",
    "    vector_queries=[\n",
    "        VectorizableTextQuery( \n",
    "            text=user_question, k_nearest_neighbors=3, fields=\"text_vector\"\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Print Results\n",
    "for result in search_results:\n",
    "    print(\"Chunk ID:\", result[\"chunk_id\"])\n",
    "    print(\"Title:\", result[\"title\"])\n",
    "    print(\"Text:\", result[\"chunk\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. RAG TIME! Generate a Response\n",
    "\n",
    "Using the retrieved documents, construct a **system prompt** and generate a response with Azure OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's collect the context from search results\n",
    "context = \"\"\n",
    "for result in search_results:\n",
    "    context += result[\"chunk\"] + \"\\n\\n\"\n",
    "\n",
    "SYSTEM_MESSAGE = f\"\"\"\n",
    "You are an AI Assistant.\n",
    "Be brief in your answers. Answer ONLY with the facts listed in the retrieved text.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "USER_MESSAGE = user_question\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=os.getenv(\"AZURE_OPENAI_CHAT_COMPLETION_DEPLOYED_MODEL_NAME\"),\n",
    "    temperature=0.7,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "        {\"role\": \"user\", \"content\": USER_MESSAGE},\n",
    "    ],\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "print(answer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Let's have a look at the metrics\n",
    "Now that we used the embedding model and the chat completion model, let's have a look at the consumption.\n",
    "First, let's explore how many tokens the previous answer generation (chat completion) using the RAG pattern required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nToken Usage of last call:\")\n",
    "print(f\"Prompt Tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Completion Tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total Tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Challenge\n",
    "Now let's experiment a little bit with our current RAG setup\n",
    "\n",
    "#### Task 1\n",
    "Find out what Healthplans Northwind Health offers in general. Use the same RAG setup as shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"What Healthplans does Northwind Health offer?\"\n",
    "## TODO: Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus Task 2:\n",
    "This is a lot of code to repeat everytime a new questions comes. Create a function that takes the user question as an input and prints out the answer on the screen as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_from_question(user_question):\n",
    "    ## TODO: your code goes here\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3:\n",
    "Now find the answers to the following questions: Is there a limit on how much can be expensed with PerksPlus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"Is there a limit on how much can be expensed with PerksPlus?\"\n",
    "## TODO: Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4:\n",
    "Let's test the system whether it avoids hallucination or answers with irrelevant information when it shouldn't. Think of a question that surely has nothing to do with the content of the Search index and test your systems ability to handle this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"your question here\"\n",
    "## TODO: Your code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5:\n",
    "Find out how many tokens you used up overall for these few questions.\n",
    "Hint: You can solve this via the Azure Portal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "1. **Environment Variables Not Loaded:** Ensure you have correctly set the `.env` file or manually export them in your terminal before running the notebook.\n",
    "1. **Authentication Issues:** If using Managed Identity, make sure your Azure identity has proper role assignments.\n",
    "1. **Search Results Are Empty:** Ensure your Azure AI Search index contains vectorized data.\n",
    "1. **OpenAI API Errors:** Verify your deployment name and API key.\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrates a **vector-based RAG pipeline** using Azure OpenAI and Azure AI Search. It retrieves relevant documents using vector search and generates responses using GPT-based chat completions. The approach improves the accuracy of AI responses by grounding them in real data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
